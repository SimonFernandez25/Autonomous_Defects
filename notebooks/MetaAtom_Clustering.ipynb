{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-Atom Defect Clustering Pipeline\n",
    "\n",
    "This notebook segments metasurface arrays into individual meta-atom images and applies multiple clustering techniques to categorize defect types:\n",
    "- **Intact**: Normal, undamaged atoms\n",
    "- **Fallen/Collapsed**: Atoms that have toppled over\n",
    "- **Missing**: Empty or nearly empty positions\n",
    "- **Misshapen/Irregular**: Deformed atoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# ML/Clustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path(os.getcwd())\n",
    "if BASE_DIR.name == 'notebooks':\n",
    "    BASE_DIR = BASE_DIR.parent\n",
    "OUTPUT_DIR = BASE_DIR / \"Meta_Atoms\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Base Directory: {BASE_DIR}\")\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image Segmentation\n",
    "\n",
    "Extract individual meta-atom tiles from the 21x21 grid arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_grid_images_simple(image_path, output_dir, grid_size=21, tile_size=32, show_grid=True):\n",
    "    \"\"\"\n",
    "    Extracts a perfect grid of tiles from a CLEAN, manually cropped array.\n",
    "    Displays the grid overlay before saving tiles.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    image_path : str\n",
    "        Path to manually cropped array image (21x21 grid).\n",
    "    output_dir : str\n",
    "        Directory to save output tiles.\n",
    "    grid_size : int\n",
    "        Number of grid rows/cols (default 21).\n",
    "    tile_size : int\n",
    "        Half-width of tile crop.\n",
    "    show_grid : bool\n",
    "        If True, display image with grid overlay.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Could not read {image_path}\")\n",
    "\n",
    "    H, W = img.shape[:2]\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "\n",
    "    # Output folder\n",
    "    array_dir = os.path.join(output_dir, base_name)\n",
    "    os.makedirs(array_dir, exist_ok=True)\n",
    "\n",
    "    # Compute grid spacing\n",
    "    x_spacing = W / grid_size\n",
    "    y_spacing = H / grid_size\n",
    "\n",
    "    # Show grid overlay\n",
    "    if show_grid:\n",
    "        img_disp = img.copy()\n",
    "\n",
    "        # Draw vertical lines\n",
    "        for c in range(grid_size + 1):\n",
    "            x = int(c * x_spacing)\n",
    "            cv2.line(img_disp, (x, 0), (x, H), (0, 255, 0), 1)\n",
    "\n",
    "        # Draw horizontal lines\n",
    "        for r in range(grid_size + 1):\n",
    "            y = int(r * y_spacing)\n",
    "            cv2.line(img_disp, (0, y), (W, y), (0, 255, 0), 1)\n",
    "\n",
    "        # Draw grid centers\n",
    "        for r in range(grid_size):\n",
    "            for c in range(grid_size):\n",
    "                cx = int((c + 0.5) * x_spacing)\n",
    "                cy = int((r + 0.5) * y_spacing)\n",
    "                cv2.circle(img_disp, (cx, cy), 3, (255, 0, 0), -1)\n",
    "\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(cv2.cvtColor(img_disp, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"Grid Overlay: {base_name}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    # Extract and save 441 tiles\n",
    "    saved = 0\n",
    "\n",
    "    for r in range(grid_size):\n",
    "        for c in range(grid_size):\n",
    "            cx = int((c + 0.5) * x_spacing)\n",
    "            cy = int((r + 0.5) * y_spacing)\n",
    "\n",
    "            x1 = max(0, cx - tile_size)\n",
    "            y1 = max(0, cy - tile_size)\n",
    "            x2 = min(W, cx + tile_size)\n",
    "            y2 = min(H, cy + tile_size)\n",
    "\n",
    "            tile = img[y1:y2, x1:x2]\n",
    "\n",
    "            fname = f\"{base_name}_{r+1},{c+1}.bmp\"\n",
    "            cv2.imwrite(os.path.join(array_dir, fname), tile)\n",
    "            saved += 1\n",
    "\n",
    "    print(f\"Saved {saved} tiles to {array_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment all three arrays\n",
    "arrays = [\"Array_1Crop.bmp\", \"Array_2Crop.bmp\", \"Array_3Crop.bmp\"]\n",
    "\n",
    "for arr in arrays:\n",
    "    arr_path = BASE_DIR / arr\n",
    "    if arr_path.exists():\n",
    "        print(f\"\\n--- Processing {arr} ---\")\n",
    "        save_grid_images_simple(\n",
    "            image_path=str(arr_path),\n",
    "            output_dir=str(OUTPUT_DIR),\n",
    "            grid_size=21,\n",
    "            tile_size=32,\n",
    "            show_grid=True\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Warning: {arr} not found at {arr_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction\n",
    "\n",
    "Extract features from each meta-atom image for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(img_gray):\n",
    "    \"\"\"\n",
    "    Extract a comprehensive feature vector from a grayscale meta-atom image.\n",
    "    \n",
    "    Returns a dictionary of features for clustering.\n",
    "    \"\"\"\n",
    "    # Gradient features (for stitching detection)\n",
    "    sobelx = cv2.Sobel(img_gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    sobely = cv2.Sobel(img_gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    mag = np.sqrt(sobelx**2 + sobely**2)\n",
    "    max_gradient = np.max(mag)\n",
    "    mean_gradient = np.mean(mag)\n",
    "    \n",
    "    # Intensity features\n",
    "    mean_intensity = np.mean(img_gray)\n",
    "    std_intensity = np.std(img_gray)\n",
    "    \n",
    "    # Binary thresholding for shape analysis\n",
    "    img_bin = cv2.adaptiveThreshold(\n",
    "        img_gray, 255,\n",
    "        cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "        cv2.THRESH_BINARY_INV,\n",
    "        21, 3\n",
    "    )\n",
    "    \n",
    "    # Contour-based features\n",
    "    contours, _ = cv2.findContours(img_bin, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    area = 0\n",
    "    perimeter = 0\n",
    "    solidity = 0\n",
    "    circularity = 0\n",
    "    hu_moments = np.zeros(7)\n",
    "    \n",
    "    if contours:\n",
    "        largest_cnt = max(contours, key=cv2.contourArea)\n",
    "        area = cv2.contourArea(largest_cnt)\n",
    "        perimeter = cv2.arcLength(largest_cnt, True)\n",
    "        \n",
    "        # Solidity\n",
    "        hull = cv2.convexHull(largest_cnt)\n",
    "        hull_area = cv2.contourArea(hull)\n",
    "        if hull_area > 0:\n",
    "            solidity = float(area) / hull_area\n",
    "        \n",
    "        # Circularity\n",
    "        if perimeter > 0:\n",
    "            circularity = (4 * np.pi * area) / (perimeter ** 2)\n",
    "        \n",
    "        # Hu Moments\n",
    "        mask = np.zeros_like(img_bin)\n",
    "        cv2.drawContours(mask, [largest_cnt], -1, 255, -1)\n",
    "        M = cv2.moments(mask)\n",
    "        hu_moments = cv2.HuMoments(M).flatten()\n",
    "    \n",
    "    # Texture features (Laplacian variance for sharpness)\n",
    "    laplacian_var = cv2.Laplacian(img_gray, cv2.CV_64F).var()\n",
    "    \n",
    "    # Histogram features\n",
    "    hist = cv2.calcHist([img_gray], [0], None, [16], [0, 256]).flatten()\n",
    "    hist = hist / hist.sum()  # Normalize\n",
    "    \n",
    "    return {\n",
    "        'mean_intensity': mean_intensity,\n",
    "        'std_intensity': std_intensity,\n",
    "        'max_gradient': max_gradient,\n",
    "        'mean_gradient': mean_gradient,\n",
    "        'area': area,\n",
    "        'perimeter': perimeter,\n",
    "        'solidity': solidity,\n",
    "        'circularity': circularity,\n",
    "        'hu_moments': hu_moments,\n",
    "        'laplacian_var': laplacian_var,\n",
    "        'histogram': hist\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_tiles():\n",
    "    \"\"\"\n",
    "    Load all tile images and extract features.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    array_names = [\"Array_1Crop\", \"Array_2Crop\", \"Array_3Crop\"]\n",
    "    \n",
    "    for array_name in array_names:\n",
    "        array_dir = OUTPUT_DIR / array_name\n",
    "        if not array_dir.exists():\n",
    "            print(f\"Warning: {array_dir} not found\")\n",
    "            continue\n",
    "            \n",
    "        files = list(array_dir.glob(\"*.bmp\"))\n",
    "        print(f\"Loading {len(files)} tiles from {array_name}\")\n",
    "        \n",
    "        for fpath in files:\n",
    "            img = cv2.imread(str(fpath), cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            # Parse row/col from filename\n",
    "            fname = fpath.stem\n",
    "            try:\n",
    "                coords = fname.split('_')[-1]\n",
    "                row, col = map(int, coords.split(','))\n",
    "            except:\n",
    "                row, col = -1, -1\n",
    "            \n",
    "            features = extract_features(img)\n",
    "            \n",
    "            all_data.append({\n",
    "                'array': array_name,\n",
    "                'filename': fpath.name,\n",
    "                'filepath': str(fpath),\n",
    "                'row': row,\n",
    "                'col': col,\n",
    "                'image': img,\n",
    "                **features\n",
    "            })\n",
    "    \n",
    "    print(f\"\\nTotal tiles loaded: {len(all_data)}\")\n",
    "    return all_data\n",
    "\n",
    "# Load all tile data\n",
    "tile_data = load_all_tiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_matrix(tile_data, use_hu=True, use_hist=True):\n",
    "    \"\"\"\n",
    "    Build a feature matrix from tile data for clustering.\n",
    "    \"\"\"\n",
    "    feature_list = []\n",
    "    \n",
    "    for tile in tile_data:\n",
    "        row = [\n",
    "            tile['mean_intensity'],\n",
    "            tile['std_intensity'],\n",
    "            tile['max_gradient'],\n",
    "            tile['mean_gradient'],\n",
    "            tile['area'],\n",
    "            tile['perimeter'],\n",
    "            tile['solidity'],\n",
    "            tile['circularity'],\n",
    "            tile['laplacian_var']\n",
    "        ]\n",
    "        \n",
    "        if use_hu:\n",
    "            # Log transform Hu moments (they vary over many orders of magnitude)\n",
    "            hu_log = -np.sign(tile['hu_moments']) * np.log10(np.abs(tile['hu_moments']) + 1e-10)\n",
    "            row.extend(hu_log)\n",
    "        \n",
    "        if use_hist:\n",
    "            row.extend(tile['histogram'])\n",
    "        \n",
    "        feature_list.append(row)\n",
    "    \n",
    "    X = np.array(feature_list)\n",
    "    X = np.nan_to_num(X, nan=0, posinf=0, neginf=0)\n",
    "    \n",
    "    return X\n",
    "\n",
    "# Build feature matrix\n",
    "X_raw = build_feature_matrix(tile_data, use_hu=True, use_hist=True)\n",
    "print(f\"Feature matrix shape: {X_raw.shape}\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dimensionality Reduction for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA for visualization and clustering\n",
    "pca = PCA(n_components=10)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Explained variance ratio (first 10 components): {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total variance explained: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# Plot explained variance\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(1, 11), pca.explained_variance_ratio_)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('PCA Explained Variance')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, 11), np.cumsum(pca.explained_variance_ratio_), 'bo-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance')\n",
    "plt.axhline(y=0.9, color='r', linestyle='--', label='90% threshold')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE for 2D visualization\n",
    "print(\"Computing t-SNE (this may take a moment)...\")\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42, n_iter=1000)\n",
    "X_tsne = tsne.fit_transform(X_pca[:, :5])  # Use first 5 PCA components\n",
    "print(\"t-SNE complete.\")\n",
    "\n",
    "# Plot t-SNE\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.5, s=10)\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.title('t-SNE Visualization of Meta-Atoms')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clustering Methods\n",
    "\n",
    "We'll try multiple clustering approaches and compare their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clustering(X, labels, method_name):\n",
    "    \"\"\"\n",
    "    Evaluate clustering quality using multiple metrics.\n",
    "    \"\"\"\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    \n",
    "    if n_clusters < 2:\n",
    "        print(f\"{method_name}: Only {n_clusters} cluster(s) found - cannot compute metrics\")\n",
    "        return None\n",
    "    \n",
    "    # Filter out noise points for metrics\n",
    "    mask = labels != -1\n",
    "    X_valid = X[mask]\n",
    "    labels_valid = labels[mask]\n",
    "    \n",
    "    if len(set(labels_valid)) < 2:\n",
    "        print(f\"{method_name}: Not enough valid clusters for metrics\")\n",
    "        return None\n",
    "    \n",
    "    silhouette = silhouette_score(X_valid, labels_valid)\n",
    "    davies_bouldin = davies_bouldin_score(X_valid, labels_valid)\n",
    "    calinski = calinski_harabasz_score(X_valid, labels_valid)\n",
    "    \n",
    "    print(f\"{method_name}:\")\n",
    "    print(f\"  Clusters: {n_clusters}\")\n",
    "    print(f\"  Silhouette Score: {silhouette:.3f} (higher is better, range [-1,1])\")\n",
    "    print(f\"  Davies-Bouldin Index: {davies_bouldin:.3f} (lower is better)\")\n",
    "    print(f\"  Calinski-Harabasz Index: {calinski:.1f} (higher is better)\")\n",
    "    \n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'n_clusters': n_clusters,\n",
    "        'silhouette': silhouette,\n",
    "        'davies_bouldin': davies_bouldin,\n",
    "        'calinski_harabasz': calinski\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal K using elbow method and silhouette\n",
    "k_range = range(2, 10)\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_pca[:, :5])\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouettes.append(silhouette_score(X_pca[:, :5], labels))\n",
    "\n",
    "# Plot elbow and silhouette\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(k_range, inertias, 'bo-')\n",
    "axes[0].set_xlabel('Number of Clusters (K)')\n",
    "axes[0].set_ylabel('Inertia')\n",
    "axes[0].set_title('Elbow Method')\n",
    "\n",
    "axes[1].plot(k_range, silhouettes, 'go-')\n",
    "axes[1].set_xlabel('Number of Clusters (K)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('Silhouette Analysis')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Best K by silhouette\n",
    "best_k = k_range[np.argmax(silhouettes)]\n",
    "print(f\"Best K by silhouette score: {best_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means with 4 clusters (for 4 defect types)\n",
    "kmeans_4 = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "labels_kmeans_4 = kmeans_4.fit_predict(X_pca[:, :5])\n",
    "\n",
    "metrics_kmeans_4 = evaluate_clustering(X_pca[:, :5], labels_kmeans_4, \"K-Means (K=4)\")\n",
    "\n",
    "# Store labels\n",
    "for i, tile in enumerate(tile_data):\n",
    "    tile['kmeans_4'] = labels_kmeans_4[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN - density based clustering\n",
    "# Try different eps values\n",
    "eps_values = [0.5, 0.7, 1.0, 1.5, 2.0]\n",
    "\n",
    "print(\"DBSCAN parameter search:\")\n",
    "for eps in eps_values:\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=10)\n",
    "    labels = dbscan.fit_predict(X_pca[:, :5])\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = (labels == -1).sum()\n",
    "    print(f\"  eps={eps}: {n_clusters} clusters, {n_noise} noise points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN with selected parameters\n",
    "dbscan = DBSCAN(eps=1.0, min_samples=10)\n",
    "labels_dbscan = dbscan.fit_predict(X_pca[:, :5])\n",
    "\n",
    "metrics_dbscan = evaluate_clustering(X_pca[:, :5], labels_dbscan, \"DBSCAN (eps=1.0)\")\n",
    "\n",
    "for i, tile in enumerate(tile_data):\n",
    "    tile['dbscan'] = labels_dbscan[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMM - soft clustering\n",
    "gmm = GaussianMixture(n_components=4, random_state=42, covariance_type='full')\n",
    "labels_gmm = gmm.fit_predict(X_pca[:, :5])\n",
    "\n",
    "metrics_gmm = evaluate_clustering(X_pca[:, :5], labels_gmm, \"GMM (4 components)\")\n",
    "\n",
    "for i, tile in enumerate(tile_data):\n",
    "    tile['gmm'] = labels_gmm[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agglomerative (Hierarchical) Clustering\n",
    "agg = AgglomerativeClustering(n_clusters=4, linkage='ward')\n",
    "labels_agg = agg.fit_predict(X_pca[:, :5])\n",
    "\n",
    "metrics_agg = evaluate_clustering(X_pca[:, :5], labels_agg, \"Agglomerative (4 clusters)\")\n",
    "\n",
    "for i, tile in enumerate(tile_data):\n",
    "    tile['agglomerative'] = labels_agg[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral Clustering\n",
    "spectral = SpectralClustering(n_clusters=4, random_state=42, affinity='nearest_neighbors', n_neighbors=10)\n",
    "labels_spectral = spectral.fit_predict(X_pca[:, :5])\n",
    "\n",
    "metrics_spectral = evaluate_clustering(X_pca[:, :5], labels_spectral, \"Spectral (4 clusters)\")\n",
    "\n",
    "for i, tile in enumerate(tile_data):\n",
    "    tile['spectral'] = labels_spectral[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization of Clustering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters_tsne(X_tsne, labels, title, ax=None):\n",
    "    \"\"\"\n",
    "    Visualize clusters in t-SNE space.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    unique_labels = sorted(set(labels))\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))\n",
    "    \n",
    "    for label, color in zip(unique_labels, colors):\n",
    "        mask = labels == label\n",
    "        label_name = 'Noise' if label == -1 else f'Cluster {label}'\n",
    "        ax.scatter(X_tsne[mask, 0], X_tsne[mask, 1], \n",
    "                   c=[color], label=label_name, alpha=0.6, s=15)\n",
    "    \n",
    "    ax.set_xlabel('t-SNE 1')\n",
    "    ax.set_ylabel('t-SNE 2')\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc='best', fontsize=8)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all clustering methods\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "visualize_clusters_tsne(X_tsne, labels_kmeans_4, 'K-Means (K=4)', axes[0, 0])\n",
    "visualize_clusters_tsne(X_tsne, labels_dbscan, 'DBSCAN', axes[0, 1])\n",
    "visualize_clusters_tsne(X_tsne, labels_gmm, 'GMM', axes[0, 2])\n",
    "visualize_clusters_tsne(X_tsne, labels_agg, 'Agglomerative', axes[1, 0])\n",
    "visualize_clusters_tsne(X_tsne, labels_spectral, 'Spectral', axes[1, 1])\n",
    "\n",
    "# Empty plot for summary\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(OUTPUT_DIR / 'clustering_comparison.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sample Images from Each Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cluster_samples(tile_data, label_key, n_samples=8):\n",
    "    \"\"\"\n",
    "    Display sample images from each cluster.\n",
    "    \"\"\"\n",
    "    labels = [tile[label_key] for tile in tile_data]\n",
    "    unique_labels = sorted(set(labels))\n",
    "    \n",
    "    n_clusters = len(unique_labels)\n",
    "    fig, axes = plt.subplots(n_clusters, n_samples, figsize=(n_samples * 1.5, n_clusters * 1.5))\n",
    "    \n",
    "    if n_clusters == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, label in enumerate(unique_labels):\n",
    "        cluster_tiles = [t for t in tile_data if t[label_key] == label]\n",
    "        samples = np.random.choice(len(cluster_tiles), min(n_samples, len(cluster_tiles)), replace=False)\n",
    "        \n",
    "        label_name = 'Noise' if label == -1 else f'Cluster {label}'\n",
    "        \n",
    "        for j in range(n_samples):\n",
    "            ax = axes[i][j] if n_clusters > 1 else axes[j]\n",
    "            if j < len(samples):\n",
    "                img = cluster_tiles[samples[j]]['image']\n",
    "                ax.imshow(img, cmap='gray')\n",
    "            ax.axis('off')\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(f'{label_name}\\n(n={len(cluster_tiles)})', fontsize=9)\n",
    "    \n",
    "    plt.suptitle(f'Cluster Samples: {label_key}', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show samples for each clustering method\n",
    "print(\"K-Means Clusters:\")\n",
    "show_cluster_samples(tile_data, 'kmeans_4')\n",
    "\n",
    "print(\"\\nGMM Clusters:\")\n",
    "show_cluster_samples(tile_data, 'gmm')\n",
    "\n",
    "print(\"\\nAgglomerative Clusters:\")\n",
    "show_cluster_samples(tile_data, 'agglomerative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cluster Statistics and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_clusters(tile_data, label_key):\n",
    "    \"\"\"\n",
    "    Analyze feature distributions within each cluster.\n",
    "    \"\"\"\n",
    "    labels = [tile[label_key] for tile in tile_data]\n",
    "    unique_labels = sorted(set(labels))\n",
    "    \n",
    "    print(f\"\\n=== Cluster Analysis for {label_key} ===\")\n",
    "    print(f\"Number of clusters: {len([l for l in unique_labels if l != -1])}\")\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        cluster_tiles = [t for t in tile_data if t[label_key] == label]\n",
    "        n = len(cluster_tiles)\n",
    "        \n",
    "        label_name = 'Noise' if label == -1 else f'Cluster {label}'\n",
    "        \n",
    "        # Calculate mean features\n",
    "        mean_intensity = np.mean([t['mean_intensity'] for t in cluster_tiles])\n",
    "        mean_area = np.mean([t['area'] for t in cluster_tiles])\n",
    "        mean_solidity = np.mean([t['solidity'] for t in cluster_tiles])\n",
    "        mean_circularity = np.mean([t['circularity'] for t in cluster_tiles])\n",
    "        mean_gradient = np.mean([t['max_gradient'] for t in cluster_tiles])\n",
    "        \n",
    "        print(f\"\\n{label_name} (n={n}):\")\n",
    "        print(f\"  Avg Intensity: {mean_intensity:.1f}\")\n",
    "        print(f\"  Avg Area: {mean_area:.1f}\")\n",
    "        print(f\"  Avg Solidity: {mean_solidity:.3f}\")\n",
    "        print(f\"  Avg Circularity: {mean_circularity:.3f}\")\n",
    "        print(f\"  Avg Max Gradient: {mean_gradient:.1f}\")\n",
    "\n",
    "# Analyze K-Means clusters\n",
    "analyze_clusters(tile_data, 'kmeans_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_features(tile_data, label_key):\n",
    "    \"\"\"\n",
    "    Plot feature distributions for each cluster as box plots.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame([{\n",
    "        'cluster': tile[label_key],\n",
    "        'intensity': tile['mean_intensity'],\n",
    "        'area': tile['area'],\n",
    "        'solidity': tile['solidity'],\n",
    "        'circularity': tile['circularity'],\n",
    "        'gradient': tile['max_gradient']\n",
    "    } for tile in tile_data])\n",
    "    \n",
    "    features = ['intensity', 'area', 'solidity', 'circularity', 'gradient']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(features), figsize=(15, 4))\n",
    "    \n",
    "    for i, feat in enumerate(features):\n",
    "        df.boxplot(column=feat, by='cluster', ax=axes[i])\n",
    "        axes[i].set_title(feat.capitalize())\n",
    "        axes[i].set_xlabel('Cluster')\n",
    "    \n",
    "    plt.suptitle(f'Feature Distributions by Cluster ({label_key})', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_cluster_features(tile_data, 'kmeans_4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of clustering metrics\n",
    "print(\"=== Clustering Method Comparison ===\")\n",
    "print()\n",
    "\n",
    "all_metrics = [m for m in [metrics_kmeans_4, metrics_dbscan, metrics_gmm, metrics_agg, metrics_spectral] if m is not None]\n",
    "\n",
    "if all_metrics:\n",
    "    import pandas as pd\n",
    "    df_metrics = pd.DataFrame(all_metrics)\n",
    "    print(df_metrics.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to CSV\n",
    "import csv\n",
    "\n",
    "csv_path = OUTPUT_DIR / 'clustering_results.csv'\n",
    "\n",
    "fieldnames = ['array', 'filename', 'row', 'col', 'mean_intensity', 'area', 'solidity', \n",
    "              'circularity', 'max_gradient', 'kmeans_4', 'dbscan', 'gmm', 'agglomerative', 'spectral']\n",
    "\n",
    "with open(csv_path, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')\n",
    "    writer.writeheader()\n",
    "    for tile in tile_data:\n",
    "        writer.writerow(tile)\n",
    "\n",
    "print(f\"Results exported to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Manual Cluster Labeling Guide\n",
    "\n",
    "Based on the feature analysis, you can interpret the clusters:\n",
    "\n",
    "- **Low Area + Low Intensity**: Likely **Missing** atoms\n",
    "- **High Solidity + Low Intensity**: Likely **Fallen/Collapsed** atoms  \n",
    "- **Low Circularity + Normal Area**: Likely **Misshapen** atoms\n",
    "- **High Area + High Circularity + Normal Intensity**: Likely **Intact** atoms\n",
    "- **High Gradient**: May indicate **Stitching errors** (image artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive cluster labeling\n",
    "def create_labeled_clusters(tile_data, label_key, cluster_names):\n",
    "    \"\"\"\n",
    "    Assign semantic labels to clusters based on feature analysis.\n",
    "    \n",
    "    cluster_names: dict mapping cluster index to label name\n",
    "    Example: {0: 'Intact', 1: 'Missing', 2: 'Fallen', 3: 'Misshapen'}\n",
    "    \"\"\"\n",
    "    for tile in tile_data:\n",
    "        cluster_id = tile[label_key]\n",
    "        tile['defect_type'] = cluster_names.get(cluster_id, 'Unknown')\n",
    "    \n",
    "    # Count defect types\n",
    "    from collections import Counter\n",
    "    defect_counts = Counter([t['defect_type'] for t in tile_data])\n",
    "    print(\"Defect Type Counts:\")\n",
    "    for defect, count in sorted(defect_counts.items()):\n",
    "        print(f\"  {defect}: {count}\")\n",
    "\n",
    "# Example: After analyzing cluster features, assign labels\n",
    "# Uncomment and modify based on your cluster analysis:\n",
    "# cluster_labels = {0: 'Intact', 1: 'Missing', 2: 'Fallen', 3: 'Misshapen'}\n",
    "# create_labeled_clusters(tile_data, 'kmeans_4', cluster_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
