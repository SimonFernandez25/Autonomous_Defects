{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layered Clustering: Sequential Defect Separation\n",
    "\n",
    "- **Layer 1**: Center Intensity -> MISSING pillars\n",
    "- **Layer 2**: Darkness + Area -> Collapsed/Dark defects\n",
    "- **Layer 3**: Stitching Detection -> Optical stitching errors (sharp lines)\n",
    "- **Layer 4**: Contextual Features -> Misshaped/Irregular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from collections import Counter\n",
    "from scipy import ndimage\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.rcParams.update({'font.size': 10, 'figure.dpi': 100})\n",
    "\n",
    "BASE_DIR = Path(os.getcwd())\n",
    "if BASE_DIR.name == 'notebooks':\n",
    "    BASE_DIR = BASE_DIR.parent\n",
    "OUTPUT_DIR = BASE_DIR / \"Meta_Atoms\"\n",
    "COLORS = ['#2ecc71', '#e74c3c', '#3498db', '#9b59b6', '#f39c12', '#1abc9c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_array(image_path, output_dir, grid_size=21, tile_size=32):\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None:\n",
    "        return\n",
    "    H, W = img.shape[:2]\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    array_dir = os.path.join(output_dir, base_name)\n",
    "    os.makedirs(array_dir, exist_ok=True)\n",
    "    x_spacing, y_spacing = W / grid_size, H / grid_size\n",
    "    for r in range(grid_size):\n",
    "        for c in range(grid_size):\n",
    "            cx, cy = int((c + 0.5) * x_spacing), int((r + 0.5) * y_spacing)\n",
    "            x1, y1 = max(0, cx - tile_size), max(0, cy - tile_size)\n",
    "            x2, y2 = min(W, cx + tile_size), min(H, cy + tile_size)\n",
    "            tile = img[y1:y2, x1:x2]\n",
    "            cv2.imwrite(os.path.join(array_dir, f\"{base_name}_{r+1},{c+1}.bmp\"), tile)\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "for arr in [\"Array_1Crop.bmp\", \"Array_2Crop.bmp\", \"Array_3Crop.bmp\"]:\n",
    "    arr_path = BASE_DIR / arr\n",
    "    if arr_path.exists() and not (OUTPUT_DIR / arr.replace('.bmp', '')).exists():\n",
    "        segment_array(str(arr_path), str(OUTPUT_DIR))\n",
    "\n",
    "tile_data = []\n",
    "for array_name in [\"Array_1Crop\", \"Array_2Crop\", \"Array_3Crop\"]:\n",
    "    array_dir = OUTPUT_DIR / array_name\n",
    "    if not array_dir.exists():\n",
    "        continue\n",
    "    for fpath in array_dir.glob(\"*.bmp\"):\n",
    "        img = cv2.imread(str(fpath), cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            continue\n",
    "        fname = fpath.stem\n",
    "        try:\n",
    "            coords = fname.split('_')[-1]\n",
    "            row, col = map(int, coords.split(','))\n",
    "        except:\n",
    "            row, col = -1, -1\n",
    "        tile_data.append({'array': array_name, 'filename': fpath.name, 'filepath': str(fpath),\n",
    "                          'row': row, 'col': col, 'image': img, 'defect_type': 'Unknown'})\n",
    "\n",
    "print(f\"Loaded {len(tile_data)} tiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_and_visualize(tiles, X, method_name, n_clusters=3, n_samples=80):\n",
    "    X_clean = np.nan_to_num(X, nan=0, posinf=0, neginf=0)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_clean)\n",
    "    n_comp = min(5, X_scaled.shape[1])\n",
    "    pca = PCA(n_components=n_comp)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_pca)\n",
    "    for i, tile in enumerate(tiles):\n",
    "        tile[f'cluster_{method_name}'] = labels[i]\n",
    "    print(f\"Computing t-SNE for {method_name}...\")\n",
    "    tsne = TSNE(n_components=2, perplexity=min(30, len(tiles)//4), random_state=42, n_iter=500)\n",
    "    X_tsne = tsne.fit_transform(X_pca[:, :min(3, n_comp)])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    for c in range(n_clusters):\n",
    "        mask = labels == c\n",
    "        axes[0].scatter(X_pca[mask, 0], X_pca[mask, 1], c=COLORS[c], label=f'C{c} (n={mask.sum()})', alpha=0.6, s=15)\n",
    "        axes[1].scatter(X_tsne[mask, 0], X_tsne[mask, 1], c=COLORS[c], label=f'C{c} (n={mask.sum()})', alpha=0.6, s=15)\n",
    "    axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})'); axes[0].set_ylabel(f'PC2')\n",
    "    axes[0].set_title('PCA'); axes[0].legend(fontsize=9)\n",
    "    axes[1].set_xlabel('t-SNE 1'); axes[1].set_ylabel('t-SNE 2'); axes[1].set_title('t-SNE'); axes[1].legend(fontsize=9)\n",
    "    counts = [np.sum(labels == c) for c in range(n_clusters)]\n",
    "    axes[2].bar(range(n_clusters), counts, color=COLORS[:n_clusters], edgecolor='black')\n",
    "    axes[2].set_xticks(range(n_clusters)); axes[2].set_title('Cluster Sizes')\n",
    "    for i, v in enumerate(counts): axes[2].text(i, v + 5, str(v), ha='center', fontweight='bold')\n",
    "    plt.suptitle(f'{method_name}', fontsize=14, fontweight='bold'); plt.tight_layout(); plt.show()\n",
    "    \n",
    "    n_cols, n_rows = 10, n_samples // 10\n",
    "    for c in range(n_clusters):\n",
    "        cluster_tiles = [t for t in tiles if t[f'cluster_{method_name}'] == c]\n",
    "        np.random.shuffle(cluster_tiles)\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols, n_rows))\n",
    "        for i, ax in enumerate(axes.flatten()):\n",
    "            if i < len(cluster_tiles): ax.imshow(cluster_tiles[i]['image'], cmap='gray')\n",
    "            ax.axis('off')\n",
    "        plt.suptitle(f'{method_name} - C{c}: {len(cluster_tiles)}', fontsize=12, fontweight='bold', color=COLORS[c])\n",
    "        plt.tight_layout(); plt.show()\n",
    "    return labels\n",
    "\n",
    "def show_extracted_defects(tiles, title, n_samples=100):\n",
    "    n_cols, n_rows = 10, (min(n_samples, len(tiles)) + 9) // 10\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols, n_rows))\n",
    "    samples = tiles.copy(); np.random.shuffle(samples)\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        if i < len(samples): ax.imshow(samples[i]['image'], cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.suptitle(f'{title}: {len(tiles)} total', fontsize=14, fontweight='bold', color='red')\n",
    "    plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# LAYER 1: Extract MISSING (Center Intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_center_intensity(tiles):\n",
    "    features = []\n",
    "    for tile in tiles:\n",
    "        img = tile['image']\n",
    "        h, w = img.shape\n",
    "        ch, cw = h // 2, w // 2\n",
    "        start_h, start_w = h // 4, w // 4\n",
    "        center = img[start_h:start_h+ch, start_w:start_w+cw]\n",
    "        mean_c, std_c, min_c, max_c = np.mean(center), np.std(center), np.min(center), np.max(center)\n",
    "        edge_mask = np.ones_like(img, dtype=bool)\n",
    "        edge_mask[start_h:start_h+ch, start_w:start_w+cw] = False\n",
    "        mean_e = np.mean(img[edge_mask])\n",
    "        features.append([mean_c, std_c, min_c, max_c, mean_e, mean_c - mean_e])\n",
    "    return np.array(features)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LAYER 1: MISSING DETECTION\")\n",
    "print(\"=\"*70)\n",
    "X_layer1 = extract_center_intensity(tile_data)\n",
    "labels_layer1 = cluster_and_visualize(tile_data, X_layer1, 'Layer1_Missing', n_clusters=3, n_samples=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_counts = Counter(labels_layer1)\n",
    "MISSING_CLUSTER = min(cluster_counts, key=cluster_counts.get)\n",
    "print(\"Cluster sizes:\")\n",
    "for c in sorted(cluster_counts.keys()):\n",
    "    print(f\"  C{c}: {cluster_counts[c]}{' <-- MISSING' if c == MISSING_CLUSTER else ''}\")\n",
    "\n",
    "missing_tiles = [t for t in tile_data if t['cluster_Layer1_Missing'] == MISSING_CLUSTER]\n",
    "layer2_tiles = [t for t in tile_data if t['cluster_Layer1_Missing'] != MISSING_CLUSTER]\n",
    "for t in missing_tiles: t['defect_type'] = 'Missing'\n",
    "print(f\"\\nMISSING: {len(missing_tiles)} | Remaining: {len(layer2_tiles)}\")\n",
    "show_extracted_defects(missing_tiles, 'LAYER 1: MISSING', n_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# LAYER 2: Extract Collapsed/Dark (Darkness + Area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_darkness_area(tiles):\n",
    "    features = []\n",
    "    for tile in tiles:\n",
    "        img = tile['image']\n",
    "        mean_int = np.mean(img)\n",
    "        dark_ratio = np.sum(img < 80) / img.size\n",
    "        very_dark_ratio = np.sum(img < 50) / img.size\n",
    "        _, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        area = cv2.contourArea(max(contours, key=cv2.contourArea)) if contours else 0\n",
    "        features.append([mean_int, dark_ratio, very_dark_ratio, area, area/img.size])\n",
    "    return np.array(features)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LAYER 2: DARKNESS + AREA\")\n",
    "print(\"=\"*70)\n",
    "X_layer2 = extract_darkness_area(layer2_tiles)\n",
    "labels_layer2 = cluster_and_visualize(layer2_tiles, X_layer2, 'Layer2_Darkness', n_clusters=3, n_samples=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_counts2 = Counter(labels_layer2)\n",
    "DEFECT2_CLUSTER = min(cluster_counts2, key=cluster_counts2.get)\n",
    "DEFECT2_NAME = 'Collapsed'\n",
    "\n",
    "print(\"Cluster sizes:\")\n",
    "for c in sorted(cluster_counts2.keys()):\n",
    "    print(f\"  C{c}: {cluster_counts2[c]}{f' <-- {DEFECT2_NAME}' if c == DEFECT2_CLUSTER else ''}\")\n",
    "\n",
    "defect2_tiles = [t for t in layer2_tiles if t['cluster_Layer2_Darkness'] == DEFECT2_CLUSTER]\n",
    "layer3_tiles = [t for t in layer2_tiles if t['cluster_Layer2_Darkness'] != DEFECT2_CLUSTER]\n",
    "for t in defect2_tiles: t['defect_type'] = DEFECT2_NAME\n",
    "print(f\"\\n{DEFECT2_NAME}: {len(defect2_tiles)} | Remaining: {len(layer3_tiles)}\")\n",
    "show_extracted_defects(defect2_tiles, f'LAYER 2: {DEFECT2_NAME.upper()}', n_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# LAYER 3: Optical Stitching Errors\n",
    "\n",
    "Detect tiles where the microscope measurement cuts off at the meta-atom boundary.\n",
    "These have a stark contrast line (horizontal or vertical) from image stitching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stitching_features(tiles):\n",
    "    \"\"\"\n",
    "    Detect stitching artifacts: sharp horizontal or vertical lines of contrast.\n",
    "    \n",
    "    Features:\n",
    "    - Max row/column gradient (sharp transitions)\n",
    "    - Edge intensity difference (left/right, top/bottom halves)\n",
    "    - Line detection via projection profiles\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    for tile in tiles:\n",
    "        img = tile['image'].astype(float)\n",
    "        h, w = img.shape\n",
    "        \n",
    "        # 1. Row-wise and column-wise mean intensity profiles\n",
    "        row_profile = np.mean(img, axis=1)  # Average intensity per row\n",
    "        col_profile = np.mean(img, axis=0)  # Average intensity per column\n",
    "        \n",
    "        # 2. Gradient of profiles (detect sharp jumps)\n",
    "        row_grad = np.abs(np.diff(row_profile))\n",
    "        col_grad = np.abs(np.diff(col_profile))\n",
    "        \n",
    "        max_row_jump = np.max(row_grad) if len(row_grad) > 0 else 0\n",
    "        max_col_jump = np.max(col_grad) if len(col_grad) > 0 else 0\n",
    "        max_jump = max(max_row_jump, max_col_jump)\n",
    "        \n",
    "        # Location of max jump (near edge = more likely stitching)\n",
    "        row_jump_loc = np.argmax(row_grad) / h if len(row_grad) > 0 else 0.5\n",
    "        col_jump_loc = np.argmax(col_grad) / w if len(col_grad) > 0 else 0.5\n",
    "        \n",
    "        # 3. Half-image intensity differences\n",
    "        top_half = np.mean(img[:h//2, :])\n",
    "        bottom_half = np.mean(img[h//2:, :])\n",
    "        left_half = np.mean(img[:, :w//2])\n",
    "        right_half = np.mean(img[:, w//2:])\n",
    "        \n",
    "        vertical_split = np.abs(top_half - bottom_half)\n",
    "        horizontal_split = np.abs(left_half - right_half)\n",
    "        max_split = max(vertical_split, horizontal_split)\n",
    "        \n",
    "        # 4. Edge strip analysis (look at border regions)\n",
    "        border = 5  # pixels from edge\n",
    "        top_strip = np.mean(img[:border, :])\n",
    "        bottom_strip = np.mean(img[-border:, :])\n",
    "        left_strip = np.mean(img[:, :border])\n",
    "        right_strip = np.mean(img[:, -border:])\n",
    "        center_region = np.mean(img[h//4:3*h//4, w//4:3*w//4])\n",
    "        \n",
    "        edge_center_diff = max(\n",
    "            np.abs(top_strip - center_region),\n",
    "            np.abs(bottom_strip - center_region),\n",
    "            np.abs(left_strip - center_region),\n",
    "            np.abs(right_strip - center_region)\n",
    "        )\n",
    "        \n",
    "        # 5. Sobel edge detection - look for strong horizontal/vertical edges\n",
    "        sobel_h = cv2.Sobel(img.astype(np.uint8), cv2.CV_64F, 0, 1, ksize=3)  # Horizontal edges\n",
    "        sobel_v = cv2.Sobel(img.astype(np.uint8), cv2.CV_64F, 1, 0, ksize=3)  # Vertical edges\n",
    "        \n",
    "        # Max edge response along rows/columns\n",
    "        max_horiz_edge = np.max(np.abs(sobel_h).mean(axis=1))\n",
    "        max_vert_edge = np.max(np.abs(sobel_v).mean(axis=0))\n",
    "        \n",
    "        # Store stitching score\n",
    "        stitching_score = max_jump + max_split + edge_center_diff\n",
    "        tile['stitching_score'] = stitching_score\n",
    "        \n",
    "        features.append([\n",
    "            max_row_jump, max_col_jump, max_jump,\n",
    "            vertical_split, horizontal_split, max_split,\n",
    "            edge_center_diff,\n",
    "            max_horiz_edge, max_vert_edge,\n",
    "            stitching_score\n",
    "        ])\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LAYER 3: STITCHING ERROR DETECTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "X_stitching = extract_stitching_features(layer3_tiles)\n",
    "print(f\"Features shape: {X_stitching.shape}\")\n",
    "print(f\"Stitching scores: min={X_stitching[:,-1].min():.1f}, max={X_stitching[:,-1].max():.1f}, mean={X_stitching[:,-1].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster to find stitching errors\n",
    "labels_stitch = cluster_and_visualize(layer3_tiles, X_stitching, 'Layer3_Stitching', n_clusters=3, n_samples=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract smallest cluster as stitching errors\n",
    "cluster_counts3 = Counter(labels_stitch)\n",
    "STITCH_CLUSTER = min(cluster_counts3, key=cluster_counts3.get)\n",
    "\n",
    "print(\"Cluster sizes:\")\n",
    "for c in sorted(cluster_counts3.keys()):\n",
    "    print(f\"  C{c}: {cluster_counts3[c]}{' <-- STITCHING' if c == STITCH_CLUSTER else ''}\")\n",
    "\n",
    "stitching_tiles = [t for t in layer3_tiles if t['cluster_Layer3_Stitching'] == STITCH_CLUSTER]\n",
    "layer4_tiles = [t for t in layer3_tiles if t['cluster_Layer3_Stitching'] != STITCH_CLUSTER]\n",
    "\n",
    "for t in stitching_tiles: t['defect_type'] = 'Stitching'\n",
    "\n",
    "print(f\"\\nSTITCHING errors: {len(stitching_tiles)} | Remaining: {len(layer4_tiles)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_extracted_defects(stitching_tiles, 'LAYER 3: STITCHING ERRORS', n_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# LAYER 4: Contextual / Relational Features\n",
    "\n",
    "Detect deviation from fabrication context for misshaped pillars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nLAYER 4: Processing {len(layer4_tiles)} remaining tiles with CONTEXTUAL features...\")\n",
    "\n",
    "# Build spatial index for neighbor lookup\n",
    "tile_index = {}\n",
    "for t in layer4_tiles:\n",
    "    key = (t['array'], t['row'], t['col'])\n",
    "    tile_index[key] = t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 1: Rotation Symmetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rotation_symmetry(tiles):\n",
    "    features = []\n",
    "    for tile in tiles:\n",
    "        img = tile['image'].astype(float)\n",
    "        h, w = img.shape\n",
    "        s = min(h, w)\n",
    "        img_sq = img[:s, :s]\n",
    "        \n",
    "        rotations = [img_sq, np.rot90(img_sq, 1), np.rot90(img_sq, 2), np.rot90(img_sq, 3)]\n",
    "        \n",
    "        l2_diffs, ssim_scores = [], []\n",
    "        for i in range(4):\n",
    "            for j in range(i+1, 4):\n",
    "                l2_diffs.append(np.sqrt(np.mean((rotations[i] - rotations[j])**2)))\n",
    "                try:\n",
    "                    ssim_scores.append(ssim(rotations[i], rotations[j], data_range=255))\n",
    "                except:\n",
    "                    ssim_scores.append(1.0)\n",
    "        \n",
    "        tile['rotation_asymmetry'] = np.mean(l2_diffs)\n",
    "        features.append([np.mean(l2_diffs), np.max(l2_diffs), 1 - np.mean(ssim_scores), 1 - np.min(ssim_scores)])\n",
    "    return np.array(features)\n",
    "\n",
    "print(\"Computing rotation symmetry...\")\n",
    "X_rotation = compute_rotation_symmetry(layer4_tiles)\n",
    "print(f\"  Rotation asymmetry range: {X_rotation[:,0].min():.2f} - {X_rotation[:,0].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 2: Neighbor Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_base_features(img):\n",
    "    mean_int, std_int = np.mean(img), np.std(img)\n",
    "    _, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        cnt = max(contours, key=cv2.contourArea)\n",
    "        area = cv2.contourArea(cnt)\n",
    "        perim = cv2.arcLength(cnt, True)\n",
    "        circ = (4 * np.pi * area) / (perim**2) if perim > 0 else 0\n",
    "    else:\n",
    "        area, circ = 0, 0\n",
    "    return np.array([mean_int, std_int, area / img.size, circ])\n",
    "\n",
    "def compute_neighbor_deviation(tiles, tile_index):\n",
    "    base_features = {(t['array'], t['row'], t['col']): compute_base_features(t['image']) for t in tiles}\n",
    "    \n",
    "    features = []\n",
    "    for t in tiles:\n",
    "        arr, r, c = t['array'], t['row'], t['col']\n",
    "        my_feat = base_features[(arr, r, c)]\n",
    "        \n",
    "        neighbor_feats = []\n",
    "        for dr in [-1, 0, 1]:\n",
    "            for dc in [-1, 0, 1]:\n",
    "                if dr == 0 and dc == 0: continue\n",
    "                key = (arr, r + dr, c + dc)\n",
    "                if key in base_features:\n",
    "                    neighbor_feats.append(base_features[key])\n",
    "        \n",
    "        if neighbor_feats:\n",
    "            deviation = my_feat - np.mean(neighbor_feats, axis=0)\n",
    "            deviation_norm = np.linalg.norm(deviation)\n",
    "        else:\n",
    "            deviation, deviation_norm = np.zeros(4), 0\n",
    "        \n",
    "        t['neighbor_deviation'] = deviation_norm\n",
    "        features.append(np.concatenate([deviation, [deviation_norm]]))\n",
    "    return np.array(features)\n",
    "\n",
    "print(\"Computing neighbor deviation...\")\n",
    "X_neighbor = compute_neighbor_deviation(layer4_tiles, tile_index)\n",
    "print(f\"  Neighbor deviation range: {X_neighbor[:,-1].min():.2f} - {X_neighbor[:,-1].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 3: Anisotropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_anisotropy(tiles):\n",
    "    features = []\n",
    "    for tile in tiles:\n",
    "        img = tile['image'].astype(float)\n",
    "        gx = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=3)\n",
    "        gy = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=3)\n",
    "        \n",
    "        Ixx = ndimage.gaussian_filter(gx * gx, sigma=2)\n",
    "        Iyy = ndimage.gaussian_filter(gy * gy, sigma=2)\n",
    "        Ixy = ndimage.gaussian_filter(gx * gy, sigma=2)\n",
    "        \n",
    "        Ixx_sum, Iyy_sum, Ixy_sum = np.sum(Ixx), np.sum(Iyy), np.sum(Ixy)\n",
    "        trace = Ixx_sum + Iyy_sum\n",
    "        det = Ixx_sum * Iyy_sum - Ixy_sum**2\n",
    "        \n",
    "        if trace > 0:\n",
    "            discriminant = max(0, trace**2 - 4*det)\n",
    "            lambda1 = (trace + np.sqrt(discriminant)) / 2\n",
    "            lambda2 = (trace - np.sqrt(discriminant)) / 2\n",
    "            aniso = (lambda1 - lambda2) / (lambda1 + lambda2 + 1e-10)\n",
    "            coherence = aniso**2\n",
    "        else:\n",
    "            aniso, coherence, lambda1, lambda2 = 0, 0, 0, 0\n",
    "        \n",
    "        orientation = 0.5 * np.arctan2(2 * Ixy_sum, Ixx_sum - Iyy_sum)\n",
    "        tile['anisotropy'] = aniso\n",
    "        features.append([aniso, coherence, np.abs(orientation), lambda1/(lambda2+1e-10)])\n",
    "    return np.array(features)\n",
    "\n",
    "print(\"Computing anisotropy...\")\n",
    "X_aniso = compute_anisotropy(layer4_tiles)\n",
    "print(f\"  Anisotropy range: {X_aniso[:,0].min():.3f} - {X_aniso[:,0].max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 4: Curvature Irregularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_curvature_irregularity(tiles):\n",
    "    features = []\n",
    "    for tile in tiles:\n",
    "        img = tile['image']\n",
    "        _, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        if contours and len(max(contours, key=cv2.contourArea)) >= 10:\n",
    "            cnt = max(contours, key=cv2.contourArea).squeeze()\n",
    "            if len(cnt.shape) == 1: cnt = cnt.reshape(-1, 2)\n",
    "            n = len(cnt)\n",
    "            if n < 10:\n",
    "                tile['curvature_irregularity'] = 0\n",
    "                features.append([0, 0, 0, 0, 0])\n",
    "                continue\n",
    "            \n",
    "            dx, dy = np.gradient(cnt[:, 0].astype(float)), np.gradient(cnt[:, 1].astype(float))\n",
    "            ddx, ddy = np.gradient(dx), np.gradient(dy)\n",
    "            denom = (dx**2 + dy**2)**1.5 + 1e-10\n",
    "            kappa = (dx * ddy - dy * ddx) / denom\n",
    "            \n",
    "            kappa_abs = np.abs(kappa)\n",
    "            kurtosis = np.mean(kappa_abs**4) / (np.mean(kappa_abs**2)**2 + 1e-10) - 3\n",
    "            max_curv, p95_curv = np.max(kappa_abs), np.percentile(kappa_abs, 95)\n",
    "            zero_crossings = np.sum(np.diff(np.sign(kappa)) != 0) / n\n",
    "            curv_var = np.var(kappa)\n",
    "            \n",
    "            tile['curvature_irregularity'] = kurtosis\n",
    "            features.append([kurtosis, max_curv, p95_curv, zero_crossings, curv_var])\n",
    "        else:\n",
    "            tile['curvature_irregularity'] = 0\n",
    "            features.append([0, 0, 0, 0, 0])\n",
    "    return np.array(features)\n",
    "\n",
    "print(\"Computing curvature irregularity...\")\n",
    "X_curvature = compute_curvature_irregularity(layer4_tiles)\n",
    "print(f\"  Kurtosis range: {X_curvature[:,0].min():.2f} - {X_curvature[:,0].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 5: Radial Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_radial_profile(img, n_bins=8):\n",
    "    h, w = img.shape\n",
    "    cy, cx = h // 2, w // 2\n",
    "    y, x = np.ogrid[:h, :w]\n",
    "    r = np.sqrt((x - cx)**2 + (y - cy)**2)\n",
    "    max_r = np.sqrt(cx**2 + cy**2)\n",
    "    bin_edges = np.linspace(0, max_r, n_bins + 1)\n",
    "    profile = []\n",
    "    for i in range(n_bins):\n",
    "        mask = (r >= bin_edges[i]) & (r < bin_edges[i+1])\n",
    "        profile.append(np.mean(img[mask]) if np.sum(mask) > 0 else 0)\n",
    "    return np.array(profile)\n",
    "\n",
    "def compute_radial_deviation(tiles):\n",
    "    profiles = np.array([compute_radial_profile(t['image'].astype(float)) for t in tiles])\n",
    "    mean_profile = np.mean(profiles, axis=0)\n",
    "    \n",
    "    features = []\n",
    "    for i, t in enumerate(tiles):\n",
    "        deviation = profiles[i] - mean_profile\n",
    "        dev_norm = np.linalg.norm(deviation)\n",
    "        t['radial_deviation'] = dev_norm\n",
    "        features.append([dev_norm, np.max(np.abs(deviation)), np.abs(deviation[0]), np.abs(deviation[-1])])\n",
    "    return np.array(features)\n",
    "\n",
    "print(\"Computing radial deviation...\")\n",
    "X_radial_dev = compute_radial_deviation(layer4_tiles)\n",
    "print(f\"  Radial deviation range: {X_radial_dev[:,0].min():.2f} - {X_radial_dev[:,0].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine + Compute Inconsistency Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_contextual = np.hstack([X_rotation, X_neighbor, X_aniso, X_curvature, X_radial_dev])\n",
    "print(f\"\\nCombined contextual features: {X_contextual.shape}\")\n",
    "\n",
    "X_contextual_clean = np.nan_to_num(X_contextual, nan=0, posinf=0, neginf=0)\n",
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X_contextual_clean)\n",
    "\n",
    "print(\"Computing LOF scores...\")\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination='auto')\n",
    "lof_labels = lof.fit_predict(X_norm)\n",
    "lof_scores = -lof.negative_outlier_factor_\n",
    "\n",
    "for i, t in enumerate(layer4_tiles):\n",
    "    t['inconsistency_score'] = lof_scores[i]\n",
    "    t['is_outlier'] = lof_labels[i] == -1\n",
    "\n",
    "n_outliers = np.sum(lof_labels == -1)\n",
    "print(f\"\\nLOF outliers: {n_outliers} ({100*n_outliers/len(layer4_tiles):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "print(\"Computing t-SNE...\")\n",
    "pca = PCA(n_components=min(10, X_norm.shape[1]))\n",
    "X_pca = pca.fit_transform(X_norm)\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42, n_iter=500)\n",
    "X_tsne = tsne.fit_transform(X_pca[:, :5])\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "sc = axes[0].scatter(X_tsne[:, 0], X_tsne[:, 1], c=lof_scores, cmap='RdYlGn_r', alpha=0.6, s=15)\n",
    "plt.colorbar(sc, ax=axes[0], label='Inconsistency')\n",
    "axes[0].set_title('t-SNE by Inconsistency')\n",
    "\n",
    "inliers, outliers = lof_labels == 1, lof_labels == -1\n",
    "axes[1].scatter(X_tsne[inliers, 0], X_tsne[inliers, 1], c='green', alpha=0.4, s=10, label=f'Inliers ({inliers.sum()})')\n",
    "axes[1].scatter(X_tsne[outliers, 0], X_tsne[outliers, 1], c='red', alpha=0.8, s=20, label=f'Outliers ({outliers.sum()})')\n",
    "axes[1].legend(); axes[1].set_title('LOF Detection')\n",
    "\n",
    "axes[2].hist(lof_scores, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[2].axvline(np.percentile(lof_scores, 90), color='red', linestyle='--', label='90th %')\n",
    "axes[2].set_xlabel('Score'); axes[2].legend(); axes[2].set_title('Score Distribution')\n",
    "\n",
    "plt.suptitle('Layer 4: Contextual Inconsistency', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top inconsistent\n",
    "sorted_tiles = sorted(layer4_tiles, key=lambda t: t['inconsistency_score'], reverse=True)\n",
    "\n",
    "fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < len(sorted_tiles):\n",
    "        ax.imshow(sorted_tiles[i]['image'], cmap='gray')\n",
    "        ax.set_title(f\"{sorted_tiles[i]['inconsistency_score']:.1f}\", fontsize=7)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Top 100 Most Inconsistent', fontsize=14, fontweight='bold', color='red')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most consistent (good reference)\n",
    "fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    idx = -(i+1)\n",
    "    if abs(idx) <= len(sorted_tiles):\n",
    "        ax.imshow(sorted_tiles[idx]['image'], cmap='gray')\n",
    "        ax.set_title(f\"{sorted_tiles[idx]['inconsistency_score']:.1f}\", fontsize=7)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Top 100 Most Consistent (Good)', fontsize=14, fontweight='bold', color='green')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LAYERED CLUSTERING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Layer 1 - MISSING: {len(missing_tiles)}\")\n",
    "print(f\"Layer 2 - {DEFECT2_NAME}: {len(defect2_tiles)}\")\n",
    "print(f\"Layer 3 - STITCHING: {len(stitching_tiles)}\")\n",
    "print(f\"Layer 4 - Remaining: {len(layer4_tiles)} (LOF outliers: {n_outliers})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
